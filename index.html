<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tianhe Wu</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianhe Wu</name>
              </p>
              <p>
                My name is Tianhe Wu, a third-year graduate student at the Intelligent Computing Lab at Tsinghua University, under the supervision of Professor <a href="https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=en">Yujiu Yang</a>. Currently, I am a research assistant at City University of Hong Kong, working with Professor <a href="https://kedema.org/">Kede Ma</a>. I am also an intern at the OPPO Research Institute, where I am mentored by Professor <a href="https://www4.comp.polyu.edu.hk/~cslzhang/">Lei Zhang</a>. I am passionate about advancing the frontiers of artificial intelligence and computer vision.
              </p>
              <p style="text-align:center">
                <a href="wth22@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=QW1JtysAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/TianheWu">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/tianhe.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tianhe.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on Multimodal Large Language Models (MLLMs), Image Quality Assessment (IQA), and Low-level Vision. Currently, most of my work is centered around IQA tasks, with future research directions primarily oriented towards MLLMs.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li><strong>2024-07:</strong> Our paper <a href="https://arxiv.org/abs/2403.10854">A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment</a> was accepted at ECCV 2024.</li>
              <li><strong>2023-09:</strong> Our paper titled <a href="https://arxiv.org/abs/2305.10983">Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment</a> has been accepted for presentation at NIPS 2023.</li>
              <li><strong>2022-04:</strong> I published my first conference paper <a href="https://arxiv.org/abs/2204.08958">MANIQA: Multi-dimension Attention Network for No-reference Image Quality Assessment.</a></li>
              <li><strong>2022-04:</strong> We won first place in the NTIRE2022 Perceptual Image Quality Assessment Challenge, winning both Track 1 (Full-Reference) and Track 2 (No-Reference) competitions.</li>
            </ul>
          </td>
        </tr>
      </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <heading>Publications & Preprints (* means equal contribution)</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mllms_for_iqa.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle> A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment
              </papertitle>
              <br>
              <strong>Tianhe Wu</strong>,
              <a href="https://scholar.google.com/citations?user=sfzOyFoAAAAJ&hl=en&oi=ao" target="_blank">Kede Ma</a>,
              <a href="https://scholar.google.com/citations?user=REWxLZsAAAAJ&hl=en" target="_blank">Jie Liang</a>,
              <a href="https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=en&oi=ao" target="_blank">Yujiu Yang</a>,
              <a href="https://scholar.google.com/citations?user=tAK5l1IAAAAJ&hl=en&oi=ao" target="_blank">Lei Zhang</a>
              <br>
              <em>ECCV 2024 </em>
              <p></p>
              <a href="https://arxiv.org/abs/2403.10854" id="MultiMon">[arXiv]</a> <a href="https://github.com/TianheWu/MLLMs-for-IQA">[Code]</a>

              <br>
              <p></p>
              <p>We conducted a systematic evaluation of MLLMs' ability to perceive image quality.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/assessor360.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle> Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment
              </papertitle>
              <br>
              <strong>Tianhe Wu*</strong>,
              <a href = "https://scholar.google.com/citations?user=2ZAstoQAAAAJ&hl=en&oi=ao">Shuwei Shi*</a>,
              <a href = "https://scholar.google.com/citations?user=mePn76IAAAAJ&hl=en&oi=ao">Haoming Cai</a>,
              <a href = "https://scholar.google.com/citations?user=EcS0L5sAAAAJ&hl=en&oi=ao">Mingdeng Cao</a>,
              <a href = "https://scholar.google.com/citations?user=mcBd8KUAAAAJ&hl=en&oi=ao">Jing Xiao</a>,
              <a href = "https://scholar.google.com/citations?user=JD-5DKcAAAAJ&hl=en&oi=ao">Yinqiang Zheng</a>,
              <a href = "https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=en&oi=ao">Yujiu Yang</a>
              <br>
              <em>NIPS 2023 </em>
              <p></p>
              <a href="https://arxiv.org/abs/2305.10983" id="MultiMon">[arXiv]</a> <a href="https://github.com/TianheWu/Assessor360">[Code]</a>

              <br>
              <p></p>
              <p>We proposed a new paradram for blind Omnidirectional IQA.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ahiq.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle> Attentions Help CNNs See Better: Attention-based Hybrid Image Quality Assessment Network
              </papertitle>
              <br>
              Shanshan Lao*,
              <a href = "https://scholar.google.com/citations?user=tFGIMJYAAAAJ&hl=en&oi=sra">Yuan Gong*</a>,
              <a href = "https://scholar.google.com/citations?user=2ZAstoQAAAAJ&hl=en&oi=ao">Shuwei Shi</a>,
              <a href = "https://scholar.google.com/citations?user=TTG2mY8AAAAJ&hl=en&oi=ao">Sidi Yang</a>,
              <strong>Tianhe Wu</strong>,
              <a href = "https://scholar.google.com/citations?user=QjVR3UUAAAAJ&hl=en&oi=ao">Jiahao Wang</a>,
              <a href = "https://scholar.google.com/citations?user=Egqp5AMAAAAJ&hl=en">Weihao Xia</a>,
              <a href = "https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=en&oi=ao">Yujiu Yang</a>
              <br>
              <em>CVPRW 2022 </em><span style="color: red;"><strong>Oral</strong></span>
              <p></p>
              <a href="https://arxiv.org/abs/2204.10485" id="MultiMon">[arXiv]</a> <a href="https://github.com/IIGROUP/AHIQ">[Code]</a>

              <br>
              <p></p>
              <p>The champion method for NTIRE2022 Full-Reference IQA.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/maniqa.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle> MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment
              </papertitle>
              <br>
              <a href = "https://scholar.google.com/citations?user=TTG2mY8AAAAJ&hl=en&oi=ao">Sidi Yang*</a>,
              <strong>Tianhe Wu*</strong>,
              <a href = "https://scholar.google.com/citations?user=2ZAstoQAAAAJ&hl=en&oi=ao">Shuwei Shi</a>,
              Shanshan Lao*,
              <a href = "https://scholar.google.com/citations?user=tFGIMJYAAAAJ&hl=en&oi=sra">Yuan Gong</a>,
              <a href = "https://scholar.google.com/citations?user=EcS0L5sAAAAJ&hl=en&oi=ao">Mingdeng Cao</a>,
              <a href = "https://scholar.google.com/citations?user=QjVR3UUAAAAJ&hl=en&oi=ao">Jiahao Wang</a>,
              <a href = "https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=en&oi=ao">Yujiu Yang</a>
              <br>
              <em>CVPRW 2022 </em><span style="color: red;"><strong>Oral</strong></span>
              <p></p>
              <a href="https://arxiv.org/abs/2204.08958" id="MultiMon">[arXiv]</a> <a href="https://github.com/IIGROUP/MANIQA">[Code]</a>

              <br>
              <p></p>
              <p>The champion method for NTIRE2022 No-Reference IQA.</p>
            </td>
          </tr>

        <style>
          .sub-table {
              width: 0;
              height: 0;
              overflow: hidden;
              position: absolute;
              left: -9999px;
              opacity: 0;
              visibility: hidden;
          }
      </style>
      
      <table class="sub-table" align="center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>
      
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
